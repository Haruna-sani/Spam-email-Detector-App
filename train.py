# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DjocqVADh0_VZl1K2junkCLTEsbSYF_d
"""

#import the necessary python libraries
import re
import nltk
import joblib
import pandas as pd
import numpy as np
import os

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Download NLTK data
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
#Clean the email
def clean_text(text):
    if isinstance(text, str):
        text = re.sub('[^a-zA-Z]', ' ', text)
        text = text.lower().split()
        text = [lemmatizer.lemmatize(word) for word in text if word not in stop_words]
        return ' '.join(text)
    return ""
#train the XGBOOST classifier using the clean email text inputs
def train_model(df, text_col='body', label_col='urls'):
    # Preprocess text
    df['clean_body'] = df[text_col].astype(str).apply(clean_text)

    # Split data
    X = df['clean_body']
    y = df[label_col]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Vectorize text (Bag of Words)
    bow_vectorizer = CountVectorizer(max_features=5000)
    X_train_bow = bow_vectorizer.fit_transform(X_train)
    X_test_bow = bow_vectorizer.transform(X_test)

    # Handle imbalance with SMOTE
    smote = SMOTE(random_state=42)
    X_train_res, y_train_res = smote.fit_resample(X_train_bow, y_train)

    # Train XGBoost classifier
    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
    model.fit(X_train_res, y_train_res)

    # Predictions and evaluation
    y_pred = model.predict(X_test_bow)
    print("Classification Report:\n", classification_report(y_test, y_pred))

    # Confusion matrix visualization
    cm = confusion_matrix(y_test, y_pred)
    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
    labels = np.array([["{0:.2f}%".format(value) for value in row] for row in cm_percent])
    class_names = ["LEGITIMATE", "SPAM"]

    plt.figure(figsize=(6,6))
    sns.heatmap(cm_percent, annot=labels, fmt='', cmap="YlGnBu",
                xticklabels=class_names, yticklabels=class_names)
    plt.title("Confusion Matrix (XGBoost)")
    plt.xlabel("Predicted Label")
    plt.ylabel("Actual Label")
    plt.tight_layout()
    plt.show()
    # Save model and vectorizer
    # Create the directory if it doesn't exist
    os.makedirs('models', exist_ok=True)
    joblib.dump(bow_vectorizer, 'models/bow_vectorizer.pkl')
    joblib.dump(model, 'models/xgb_spam_model.pkl')

if __name__ == "__main__":
    # Load your dataset here
    df = pd.read_csv("/content/Nigerian_Fraud (1).csv")  # replace with your path
    train_model(df)